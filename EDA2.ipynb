{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "TRAIN_COLUMNS = dict()\n",
    "\n",
    "def nan_to_tuple(x):\n",
    "    return x if x else tuple()\n",
    "\n",
    "def add_dummies_test(df: pd.DataFrame):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Keywords: (10003 unique)\n",
    "    dummy_keywords = pd.DataFrame(mlb.fit_transform(df['Keywords.id'].apply(nan_to_tuple)),\n",
    "                                    columns=[f\"keyword_{kw_id}\" for kw_id in mlb.classes_], \n",
    "                                    index=df.index)\n",
    "    # keyword_hist = dummy_keywords.sum()\n",
    "    # dummy_keywords = dummy_keywords.loc[:, keyword_hist > keyword_hist.quantile(0.99) ] # Top 1% keywords\n",
    "    dummy_keywords = dummy_keywords[TRAIN_COLUMNS['dummy_keywords']]\n",
    "    df = pd.concat([df, dummy_keywords], axis=1)\n",
    "\n",
    "    # Genres:\n",
    "    dummy_genres = pd.DataFrame(mlb.fit_transform(df.genres.apply(nan_to_tuple)),\n",
    "                            columns=[f\"genre_{cl}\" for cl in mlb.classes_], \n",
    "                            index=df.index)\n",
    "    df = pd.concat([df, dummy_genres], axis=1)\n",
    "\n",
    "    # Companies:\n",
    "    dummy_companies = pd.DataFrame(mlb.fit_transform(df['production_companies.id'].apply(nan_to_tuple)),\n",
    "                                columns=[f\"company_{cl}\" for cl in mlb.classes_], \n",
    "                                index=df.index)\n",
    "    dummy_companies = dummy_companies[TRAIN_COLUMNS['dummy_companies']]\n",
    "    df = pd.concat([df, dummy_companies], axis=1) # Maybe biggest company size is enough...   \n",
    "\n",
    "    # Production countries:\n",
    "    dummy_countries =pd.DataFrame(mlb.fit_transform(df.production_countries.apply(nan_to_tuple)),\n",
    "                                columns=[f\"country_{cl}\" for cl in mlb.classes_], \n",
    "                                index=df.index)\n",
    "    dummy_countries = dummy_countries[TRAIN_COLUMNS['dummy_countries']]                                \n",
    "    df = pd.concat([df, dummy_countries], axis=1)\n",
    "\n",
    "    # Spoken Languages:\n",
    "    dummy_lang = pd.DataFrame(mlb.fit_transform(df.spoken_languages.apply(nan_to_tuple)),\n",
    "                            columns=[f\"spoken_lang_{cl}\" for cl in mlb.classes_], \n",
    "                            index=df.index)\n",
    "    dummy_lang = dummy_lang[TRAIN_COLUMNS['dummy_lang']]                            \n",
    "    df = pd.concat([df, dummy_lang], axis=1)\n",
    "\n",
    "    # Cast:\n",
    "    # dummy_cast = pd.DataFrame(mlb.fit_transform(df['cast.id'].apply(nan_to_tuple)),\n",
    "    #                             columns=[f\"cast_{cl}\" for cl in mlb.classes_], \n",
    "    #                             index=df.index)\n",
    "    # cast_hist = dummy_cast.sum()                                \n",
    "    # dummy_cast = dummy_cast.loc[:, cast_hist > 20] # Removes cast with lower movies than 20  \n",
    "    # df = pd.concat([df, dummy_cast], axis=1)\n",
    "\n",
    "    # Original Language dummy:\n",
    "    dummy_orig_lang = pd.get_dummies(df.original_language, prefix=\"original_lang\")\n",
    "    dummy_orig_lang = dummy_orig_lang[TRAIN_COLUMNS['dummy_orig_lang']]\n",
    "    df = pd.concat([df, dummy_orig_lang], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_dummies_train(df: pd.DataFrame):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Keywords: (10003 unique)\n",
    "    dummy_keywords = pd.DataFrame(mlb.fit_transform(df['Keywords.id'].apply(nan_to_tuple)),\n",
    "                                    columns=[f\"keyword_{kw_id}\" for kw_id in mlb.classes_], \n",
    "                                    index=df.index)\n",
    "    # keyword_hist = dummy_keywords.sum()\n",
    "    # dummy_keywords = dummy_keywords.loc[:, keyword_hist > keyword_hist.quantile(0.99) ] # Top 1% keywords\n",
    "    dummy_keywords = dummy_keywords[dummy_keywords.sum().nlargest(20).index]\n",
    "    df = pd.concat([df, dummy_keywords], axis=1)\n",
    "    TRAIN_COLUMNS['dummy_keywords'] = dummy_keywords.columns\n",
    "\n",
    "    # Genres:\n",
    "    dummy_genres = pd.DataFrame(mlb.fit_transform(df.genres.apply(nan_to_tuple)),\n",
    "                            columns=[f\"genre_{cl}\" for cl in mlb.classes_], \n",
    "                            index=df.index)\n",
    "    df = pd.concat([df, dummy_genres], axis=1)\n",
    "\n",
    "    # Companies:\n",
    "    dummy_companies = pd.DataFrame(mlb.fit_transform(df['production_companies.id'].apply(nan_to_tuple)),\n",
    "                                columns=[f\"company_{cl}\" for cl in mlb.classes_], \n",
    "                                index=df.index)\n",
    "    dummy_companies = dummy_companies[dummy_companies.sum().nlargest(10).index]\n",
    "    df = pd.concat([df, dummy_companies], axis=1) # Maybe biggest company size is enough...   \n",
    "    TRAIN_COLUMNS['dummy_companies'] = dummy_companies.columns\n",
    "\n",
    "    # Production countries:\n",
    "    dummy_countries =pd.DataFrame(mlb.fit_transform(df.production_countries.apply(nan_to_tuple)),\n",
    "                                columns=[f\"country_{cl}\" for cl in mlb.classes_], \n",
    "                                index=df.index)\n",
    "    dummy_countries = dummy_countries[dummy_countries.sum().nlargest(10).index]                                \n",
    "    df = pd.concat([df, dummy_countries], axis=1)\n",
    "    TRAIN_COLUMNS['dummy_countries'] = dummy_countries.columns\n",
    "\n",
    "    # Spoken Languages:\n",
    "    dummy_lang = pd.DataFrame(mlb.fit_transform(df.spoken_languages.apply(nan_to_tuple)),\n",
    "                            columns=[f\"spoken_lang_{cl}\" for cl in mlb.classes_], \n",
    "                            index=df.index)\n",
    "    dummy_lang = dummy_lang[dummy_lang.sum().nlargest(10).index]                            \n",
    "    df = pd.concat([df, dummy_lang], axis=1)\n",
    "    TRAIN_COLUMNS['dummy_lang'] = dummy_lang.columns\n",
    "\n",
    "    # Cast:\n",
    "    # dummy_cast = pd.DataFrame(mlb.fit_transform(df['cast.id'].apply(nan_to_tuple)),\n",
    "    #                             columns=[f\"cast_{cl}\" for cl in mlb.classes_], \n",
    "    #                             index=df.index)\n",
    "    # cast_hist = dummy_cast.sum()                                \n",
    "    # dummy_cast = dummy_cast.loc[:, cast_hist > 20] # Removes cast with lower movies than 20  \n",
    "    # df = pd.concat([df, dummy_cast], axis=1)\n",
    "\n",
    "    # Original Language dummy:\n",
    "    dummy_orig_lang = pd.get_dummies(df.original_language, prefix=\"original_lang\")\n",
    "    dummy_orig_lang = dummy_orig_lang[dummy_orig_lang.sum().nlargest(10).index]\n",
    "    df = pd.concat([df, dummy_orig_lang], axis=1)\n",
    "    TRAIN_COLUMNS['dummy_orig_lang'] = dummy_orig_lang.columns\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_and_max(collection, mapping_dict):\n",
    "    return max(map(mapping_dict.get, collection)) if collection else None\n",
    "\n",
    "def eval_or_nan(obj):\n",
    "    if obj and pd.notnull(obj) and isinstance(obj, str):\n",
    "        return eval(obj)\n",
    "    return None\n",
    "\n",
    "def map_attribute(obj, attribute_name: str):\n",
    "    if obj:\n",
    "        iterable = eval(obj) if isinstance(obj, str) else obj\n",
    "        return tuple(map(lambda x: x.get(attribute_name, None), iterable))\n",
    "    return None\n",
    "\n",
    "def smart_len(x, split_char= None):\n",
    "    if split_char:\n",
    "        return len(x.split(\" \")) if pd.notnull(x) else 0\n",
    "    return len(x) if pd.notnull(x) else 0\n",
    "\n",
    "def features_flattening(df: pd.DataFrame):\n",
    "    df['belongs_to_collection'] = df.belongs_to_collection.apply(eval_or_nan)\n",
    "    df['belongs_to_collection.id'] = df.belongs_to_collection\\\n",
    "                                            .apply(lambda x: None if pd.isna(x) else x['id']).astype('Int64')\n",
    "\n",
    "\n",
    "    df['genres'] = df.genres.apply(lambda gs: tuple(g['name'] for g in eval(gs)))\n",
    "\n",
    "    df['production_companies'] = df.production_companies.apply(eval_or_nan)\n",
    "    df['production_companies.id'] = df.production_companies\\\n",
    "                                            .apply(lambda companies: map_attribute(companies, 'id'))\n",
    "    df['production_companies.origin_country'] = df.production_companies\\\n",
    "                                            .apply(lambda companies: map_attribute(companies, 'origin_country'))\n",
    "\n",
    "    df['production_countries'] = df.production_countries.apply(lambda countries: map_attribute(countries, 'iso_3166_1'))\n",
    "\n",
    "    df['release_date'] = pd.to_datetime(df.release_date)\n",
    "    df['release_month'] = df.release_date.dt.month\n",
    "    df['release_quarter'] = df.release_date.dt.quarter\n",
    "    df['release_year'] = df.release_date.dt.year\n",
    "\n",
    "    df['spoken_languages'] = df.spoken_languages.apply(lambda langs: map_attribute(langs, 'iso_639_1'))\n",
    "\n",
    "    df['Keywords'] = df.Keywords.apply(eval_or_nan)\n",
    "    df['Keywords.id'] =df.Keywords.apply(lambda keywords: map_attribute(keywords, 'id')) # TODO: Maybe keep words?\n",
    "\n",
    "    df['cast'] = df.cast.apply(eval_or_nan)\n",
    "    df['cast.id'] = df.cast.apply(lambda actors: map_attribute(actors, 'id'))\n",
    "    df['cast.gender'] = df.cast.apply(lambda actors: map_attribute(actors, 'gender')) # Gender ratio\n",
    "\n",
    "    df['crew'] = df.crew.apply(eval)\n",
    "    df['crew.id'] = df.crew.apply(lambda crew: map_attribute(crew, 'id'))\n",
    "    df['crew.gender'] = df.crew.apply(lambda crew: map_attribute(crew, 'gender')) # Gender ratio\n",
    "    df['crew.department'] = df.crew.apply(lambda crew: map_attribute(crew, 'department')) # Dept size\n",
    "    \n",
    "    df.drop(['crew', 'cast', 'Keywords', 'belongs_to_collection', 'release_date'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def missing_value_imputation(df: pd.DataFrame):\n",
    "    from sklearn.impute import KNNImputer\n",
    "\n",
    "    df.budget.fillna(0, inplace=True)\n",
    "    df.budget.replace(0, -1, inplace= True)\n",
    "    \n",
    "    df.runtime.fillna(0, inplace=True)\n",
    "    df.runtime.replace(0, -1, inplace= True)\n",
    "\n",
    "    imputer = KNNImputer(missing_values= -1)\n",
    "    imputed = imputer.fit_transform(df)\n",
    "\n",
    "    return pd.DataFrame(imputed, columns=df.columns,index=df.index)\n",
    "\n",
    "def get_element_frequency(df, attribute):\n",
    "    return Counter(df[attribute].dropna().sum())\n",
    "\n",
    "# Gender actor ratio: 0 is unspecified, 1 is female, and 2 is male\n",
    "def genders_ratio(genders):\n",
    "    arr = np.array(genders)\n",
    "    males = (arr == 1).sum()\n",
    "    females = (arr == 2).sum()\n",
    "    if males or females:\n",
    "        return males / (females + males)\n",
    "    return 0\n",
    "\n",
    "def logarithmic_scaling(df):\n",
    "    df['budget'] = df.budget.transform(np.log1p)\n",
    "    df['revenue'] = df.revenue.transform(np.log1p)\n",
    "    return df\n",
    "\n",
    "def feature_extraction(df: pd.DataFrame, test: bool):\n",
    "    removed_columns = ['backdrop_path', 'homepage', 'poster_path', 'imdb_id', 'video']\n",
    "    X = df[[col for col in df.columns if col not in removed_columns]].copy().set_index('id')\n",
    "\n",
    "    # Log scale big numbers:\n",
    "    X = logarithmic_scaling(X)\n",
    "\n",
    "    Y = X['revenue'].copy()\n",
    "    X.drop('revenue', axis=1, inplace=True)\n",
    "\n",
    "    # Flatten and extract:\n",
    "    X = features_flattening(X)\n",
    "    \n",
    "    # Collection size:\n",
    "    X['collection_size'] = X.groupby('belongs_to_collection.id')['belongs_to_collection.id']\\\n",
    "                                    .transform('count').fillna(0).astype(int).copy()\n",
    "\n",
    "    # Company with most productions: (In data)\n",
    "    company_size_dict = get_element_frequency(X, 'production_companies.id') # {company_id : company_size}\n",
    "    X['biggest_production_company_size'] = X['production_companies.id']\\\n",
    "                                        .apply(lambda companies: map_and_max(companies, company_size_dict))\\\n",
    "                                        .fillna(0).astype(int)\n",
    "\n",
    "    # Country with most production companies\n",
    "    id_country_set = set(X.production_companies\n",
    "                        .apply(lambda xs: [(x['id'], x['origin_country']) for x in xs if x['origin_country']])\n",
    "                        .sum())\n",
    "    company_per_country = Counter(country for comp_id, country in id_country_set)\n",
    "    company_per_country[''] = 0 # Update no-countries to 0\n",
    "    X['most_companies_country_size'] = X['production_companies.origin_country']\\\n",
    "                                    .apply(lambda companies: map_and_max(companies, company_per_country))\\\n",
    "                                    .fillna(0).astype(int)\n",
    "    \n",
    "    # Largest production country size:\n",
    "    country_size_dict = get_element_frequency(X, 'production_countries') # {country : movie_count}\n",
    "    X['most_productions_country_size'] = X['production_countries']\\\n",
    "                                        .apply(lambda countries: map_and_max(countries, country_size_dict))\\\n",
    "                                        .fillna(0).astype(int)\n",
    "    # Males/ Females+Males ratio:\n",
    "    X['cast.gender_ratio'] = X['cast.gender'].apply(genders_ratio)\n",
    "\n",
    "    # Num of spoken languages\n",
    "    X['spoken_lang_num'] = X.spoken_languages.apply(len)\n",
    "\n",
    "    # Word\\Char count:\n",
    "    X['overview_word_count'] = X.overview.apply(lambda x: smart_len(x, ' ')) # Overview word-count\n",
    "    X['tagline_char_count'] = X.tagline.apply(smart_len) # tagline character-count\n",
    "    X['title_char_count'] = X.title.apply(smart_len) # title character-count\n",
    "\n",
    "    # Cast size:\n",
    "    X['cast_size'] = X['cast.id'].apply(smart_len)\n",
    "\n",
    "    # Crew size:\n",
    "    X['crew_size'] = X['crew.id'].apply(smart_len)\n",
    "\n",
    "    # Dept. size:\n",
    "    dept_size_df = X['crew.department'].apply(lambda x: pd.Series(Counter(x)))\\\n",
    "                        .add_suffix('_depart_size')\\\n",
    "                        .astype('Int64')\n",
    "    dept_size_df.dropna(axis=1, thresh= dept_size_df.shape[0] * 0.20, inplace=True) # Drop columns with less than 20% data\n",
    "    dept_size_df.fillna(0, inplace=True) # Missing value imputation with 0\n",
    "    X = pd.concat([X, dept_size_df], axis=1)\n",
    "\n",
    "    # Mean by years:\n",
    "    mean_by_year = X.groupby(\"release_year\")[['runtime', 'budget', 'popularity']]\\\n",
    "                        .aggregate('mean')\\\n",
    "                        .rename(columns= {  'runtime' : 'avg_runtime_by_year',\n",
    "                                            'budget' : 'avg_budget_by_year',\n",
    "                                            'popularity' : 'avg_popularity_by_year'})\n",
    "    X = X.join(mean_by_year, how='left', on='release_year')\n",
    "\n",
    "    # Original title changed:\n",
    "    X['title_changed'] = (X['original_title'] != X['title'])\n",
    "\n",
    "    # Add dummies:\n",
    "    if test:\n",
    "        X = add_dummies_test(X)\n",
    "    else:\n",
    "        X = add_dummies_train(X)\n",
    "\n",
    "    # Drop unneccesry fields:\n",
    "    tuple_fields = ['genres', 'spoken_languages', 'production_countries', 'production_companies.id', 'Keywords.id', 'cast.id', 'cast.gender', 'crew.id', 'crew.gender', 'crew.department', 'belongs_to_collection.id', 'production_companies', 'production_companies.origin_country']\n",
    "    text_fields = ['original_language', 'original_title', 'overview', 'status', 'tagline', 'title']\n",
    "\n",
    "    X.drop(tuple_fields+text_fields, axis=1, inplace=True)\n",
    "\n",
    "    X = missing_value_imputation(X)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('data/train.tsv',delimiter='\\t')\n",
    "train_X, train_Y = feature_extraction(train_raw, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv('data/test.tsv',delimiter='\\t')\n",
    "test_X, test_Y = feature_extraction(test_raw, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['keyword_818', 'keyword_187056', 'keyword_9826', 'keyword_179431',\n       'keyword_242', 'keyword_14819', 'keyword_5565', 'keyword_9672',\n       'keyword_9663', 'keyword_10183', 'keyword_9748', 'keyword_6054',\n       'keyword_9673', 'keyword_970', 'keyword_6149', 'keyword_6075',\n       'keyword_380', 'keyword_4565', 'keyword_13130', 'keyword_179430',\n       'genre_Action', 'genre_Adventure', 'genre_Animation', 'genre_Comedy',\n       'genre_Crime', 'genre_Documentary', 'genre_Drama', 'genre_Family',\n       'genre_Fantasy', 'genre_History', 'genre_Horror', 'genre_Music',\n       'genre_Mystery', 'genre_Romance', 'genre_Science Fiction',\n       'genre_TV Movie', 'genre_Thriller', 'genre_War', 'genre_Western',\n       'company_174', 'company_33', 'company_4', 'company_5', 'company_25',\n       'company_21', 'company_12', 'company_104', 'company_9195', 'company_2',\n       'country_US', 'country_GB', 'country_FR', 'country_DE', 'country_CA',\n       'country_IN', 'country_JP', 'country_IT', 'country_ES', 'country_AU',\n       'spoken_lang_en', 'spoken_lang_fr', 'spoken_lang_es', 'spoken_lang_de',\n       'spoken_lang_it', 'spoken_lang_ru', 'spoken_lang_ja', 'spoken_lang_hi',\n       'spoken_lang_zh', 'spoken_lang_ar', 'original_lang_en',\n       'original_lang_fr', 'original_lang_hi', 'original_lang_ja',\n       'original_lang_es', 'original_lang_ru', 'original_lang_ko',\n       'original_lang_it', 'original_lang_zh', 'original_lang_cn'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_X.columns[34:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "set()"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    " set( test_X.columns ) - set( train_X.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "set()"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "diff = set( train_X.columns ) - set( test_X.columns )\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.16380213633010557"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\n",
    "\n",
    "clf = RandomForestRegressor().fit(train_X, train_Y)\n",
    "pred = clf.predict(test_X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "np.sqrt(mean_squared_log_error(test_Y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
    }
   ],
   "source": [
    "# HyperParameters:\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = list(range(100, 4000, 100))\n",
    "\n",
    "criterion = ['mse', 'mae']\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2', 0.2, 0.4, 0.6, 0.8]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = list(range(10, 200, 10)) + [None]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8, 16]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = { 'n_estimators': n_estimators,\n",
    "                'criterion': criterion,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = rf_random.best_estimator_.predict(test_X)\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "np.sqrt(mean_squared_log_error(test_Y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 3 folds for each of 4128768 candidates, totalling 12386304 fits\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2 \n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2, total=  58.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2 \n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.2s remaining:    0.0s\n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2, total=   0.7s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=2, total=   1.0s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4\n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=4, total=   0.8s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16, total=   0.7s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16, total=   0.8s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=16, total=   0.8s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32, total=   0.8s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=32, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=64 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=64, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=64 \n[CV]  l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=64, total=   0.2s\n[CV] l2_regularization=0, learning_rate=0.001, loss=least_squares, max_bins=255, max_depth=None, max_iter=100, max_leaf_nodes=None, min_samples_leaf=64 \n"
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.3 MiB for an array with shape (3565, 1738) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-67e8c60ef8f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mrf_random\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Fit the random search model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mrf_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;31m# check whether we should index with loc or iloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkey_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2127\u001b[0m         \u001b[1;31m# a list of integers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2129\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         \u001b[1;31m# a single integer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2109\u001b[0m         \"\"\"\n\u001b[0;32m   2110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2113\u001b[0m             \u001b[1;31m# re-raise with different error message\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   3407\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3408\u001b[0m         \"\"\"\n\u001b[1;32m-> 3409\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3410\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3394\u001b[0m         new_data = self._data.take(\n\u001b[1;32m-> 3395\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3396\u001b[0m         )\n\u001b[0;32m   3397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m         return self.reindex_indexer(\n\u001b[1;32m-> 1394\u001b[1;33m             \u001b[0mnew_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m         )\n\u001b[0;32m   1396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                     ),\n\u001b[0;32m   1266\u001b[0m                 )\n\u001b[1;32m-> 1267\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m             ]\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                     ),\n\u001b[0;32m   1266\u001b[0m                 )\n\u001b[1;32m-> 1267\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m             ]\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m         new_values = algos.take_nd(\n\u001b[1;32m-> 1291\u001b[1;33m             \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m         )\n\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1655\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 47.3 MiB for an array with shape (3565, 1738) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "loss = ['least_squares', 'least_absolute_deviation']\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1]\n",
    "max_iter = [100, 200, 500, 1000, 1500, 1800, 2000, 3000]\n",
    "max_leaf_nodes = [None, 31, 41, 53, 67, 79, 89, 97]\n",
    "max_depth = [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "min_samples_leaf = [2, 4, 16, 32, 64, 128, 256]\n",
    "l2_regularization = [0, 0.001,  0.01, 0.1, 0.2, 0.4, 0.6, 1]\n",
    "max_bins = [255, 2**7 - 1, 2**6 - 1, 2**5 - 1, 2**4 - 1, 2**3 - 1]\n",
    "\n",
    "random_grid = {'loss': loss,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_iter': max_iter,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'l2_regularization': l2_regularization,\n",
    "               'max_bins': max_bins}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "hgb = HistGradientBoostingRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = GridSearchCV(estimator = hgb, param_grid = random_grid, cv = 3, verbose=2, n_jobs = 1, pre_dispatch='2*n_jobs')\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "train_X.budget.replace(0, -1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(missing_values= -1)\n",
    "imputed = imputer.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_X = pd.DataFrame(imputed, columns=train_X.columns,index=train_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_X.budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitbasecondad107695b9a69403d88bbb2cd5dc32396",
   "display_name": "Python 3.7.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}